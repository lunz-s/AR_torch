{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from data import fMRIData, fMRIDataEval, LUNA, LUNAEval\n",
    "from operators import MRI, Operator\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from networks import ResNet, ConvNet, ConvenientModel\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad as torch_grad\n",
    "from utils import Norm, mkdir\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves\n",
    "BASE_PATH = '/store/CCIMI/sl767/Experiments/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 'CT_LUNA_Sobolev_-1_Exp_1'\n",
    "# Data Setup\n",
    "# DATASET = 'fMRI'\n",
    "# MODALITY= 'MRI'\n",
    "# NOISE_LEVEL = 1e-3\n",
    "# N_ANGLES = 60\n",
    "DATASET = 'LUNA'\n",
    "MODALITY= 'CT'\n",
    "NOISE_LEVEL = 2e-3\n",
    "N_ANGLES = 100\n",
    "INITIAL_MINIMIZATION = False\n",
    "\n",
    "# Network training\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 16\n",
    "TRAINING_STEPS = 30000\n",
    "# Regulariser architecture\n",
    "CHANNELS = [1, 64, 128, 128]\n",
    "DOWNSAMPLING = [False, True, True]\n",
    "SOBOLEV = -1\n",
    "PIXEL_SIZE = 1\n",
    "# regularisation parameter for gradient penalty\n",
    "MU = 10\n",
    "GAMMA = 10\n",
    "# scales gradient norm with average image norm. Normalizes expected distributional distance to 1.\n",
    "AVERAGE_IMAGE_NORM = 95\n",
    "AVERAGE_DUAL_NORM = 1400\n",
    "\n",
    "# Noise norms:\n",
    "AVERAGE_NOISE_NORM = 15\n",
    "AVERAGE_NOISE_DUAL = 15\n",
    "\n",
    "# solving the variational problem\n",
    "LAMBDA = 1500\n",
    "N_STEPS = 80\n",
    "STEP_SIZE = .1\n",
    "THRESHOLDING = False\n",
    "# tracking\n",
    "N_IMAGES_LOGGING = 1\n",
    "TRACKING_FREQ = 100\n",
    "\n",
    "parameters = {'DATASET': DATASET,\n",
    "              'MODALITY': MODALITY,\n",
    "              'NOISE_LEVEL': NOISE_LEVEL, \n",
    "              'N_ANGLES': N_ANGLES,\n",
    "              'INITIAL_MINIMIZATION': INITIAL_MINIMIZATION,\n",
    "              'LEARNING_RATE': LEARNING_RATE, \n",
    "              'BATCH_SIZE': BATCH_SIZE, \n",
    "              'TRAINING_STEPS': TRAINING_STEPS, \n",
    "              'CHANNELS': CHANNELS,\n",
    "              'DOWNSAMPLING': DOWNSAMPLING,\n",
    "              'SOBOLEV': SOBOLEV,\n",
    "              'PIXEL_SIZE': PIXEL_SIZE,\n",
    "              'MU': MU,\n",
    "              'GAMMA': GAMMA,\n",
    "              'LAMBDA': LAMBDA,\n",
    "              'N_STEPS': N_STEPS,\n",
    "              'STEP_SIZE': STEP_SIZE,\n",
    "              'THRESHOLDING': THRESHOLDING, \n",
    "              'N_IMAGES_LOGGING': N_IMAGES_LOGGING,\n",
    "              'TRACKING_FREQ': TRACKING_FREQ\n",
    "#               'AVERAGE_IMAGE_NORM': AVERAGE_IMAGE_NORM, \n",
    "#               'AVERAGE_DUAL_NORM': AVERAGE_DUAL_NORM,\n",
    "#               'AVERAGE_NOISE_NORM': AVERAGE_NOISE_NORM,\n",
    "#               'AVERAGE_NOISE_DUAL': AVERAGE_NOISE_DUAL,\n",
    "             }\n",
    "\n",
    "OVERWRITE = True\n",
    "mkdir(BASE_PATH + EXPERIMENT)\n",
    "if os.path.exists(BASE_PATH + EXPERIMENT + '/param.json') and not OVERWRITE:\n",
    "    print('Parameters found. Not overwriting.')\n",
    "else:\n",
    "    if not os.path.exists(BASE_PATH + EXPERIMENT + '/param.json'):\n",
    "        print('Parameters not found. Creating them')\n",
    "    else:\n",
    "        print('Parameters found. OVERWRITING!')\n",
    "    with open(BASE_PATH + EXPERIMENT + '/param.json',\"w\") as f:\n",
    "        json.dump(parameters, f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE = True\n",
    "mkdir(BASE_PATH + EXPERIMENT)\n",
    "if os.path.exists(BASE_PATH + EXPERIMENT + '/param.json') and not OVERWRITE:\n",
    "    print('Parameters found. Not overwriting.')\n",
    "else:\n",
    "    if not os.path.exists(BASE_PATH + EXPERIMENT + '/param.json'):\n",
    "        print('Parameters not found. Creating them')\n",
    "    else:\n",
    "        print('Parameters found. OVERWRITING!')\n",
    "    with open(BASE_PATH + EXPERIMENT + '/param.json',\"w\") as f:\n",
    "        json.dump(parameters, f)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Norm(s = SOBOLEV, c = 1/PIXEL_SIZE)\n",
    "dual = norm.dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = SummaryWriter(BASE_PATH+f'{EXPERIMENT}/Logs/')\n",
    "regulariser = ResNet(channels=CHANNELS, downsamples=DOWNSAMPLING, base_path=BASE_PATH, exp_name=EXPERIMENT).cuda()\n",
    "if MODALITY == 'MRI':\n",
    "    operator = MRI(n_directions=N_ANGLES)\n",
    "elif MODALITY == 'CT':\n",
    "    operator = CT(n_angles=N_ANGLES)\n",
    "if DATASET == 'fMRI':\n",
    "    data = DataLoader(fMRIData(), batch_size=BATCH_SIZE)\n",
    "    val_data = DataLoader(fMRIDataEval(), batch_size=BATCH_SIZE)\n",
    "elif DATASET == 'LUNA':\n",
    "    data = DataLoader(LUNA(), batch_size=BATCH_SIZE)\n",
    "    val_data = DataLoader(LUNAEval(), batch_size=BATCH_SIZE)\n",
    "optimizer = optim.Adam(regulariser.parameters(), lr=LEARNING_RATE)\n",
    "operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = next(iter(data)).cuda()\n",
    "fbp, y = get_training_data(gt)\n",
    "print(torch.mean(norm(gt-fbp)))\n",
    "print(torch.mean(dual(gt-fbp)))\n",
    "plt.imshow(gt.cpu()[1,0,...])\n",
    "plt.show()\n",
    "plt.imshow(fbp.cpu()[1,0,...])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(gt):\n",
    "    y = operator.add_noise(operator.forward_torch(gt), NOISE_LEVEL)\n",
    "    x_0 = operator.inverse_torch(y)\n",
    "    # potentially add a solution of variational problem at this point\n",
    "    if INITIAL_MINIMIZATION:\n",
    "        x_0 = solve_variational_problem(x_0, y, 10, .1, 0, tracking=None)\n",
    "    return x_0, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2(x):\n",
    "    return torch.mean(torch.sqrt(torch.sum(x ** 2, dim=(1,2,3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_lambda():\n",
    "    gt = next(iter(data)).cuda()\n",
    "    _, y = get_training_data(gt)\n",
    "    norm_data = dual(operator.adjoint_torch(operator.forward_torch(gt) - y))\n",
    "    return norm_data.cpu().numpy()\n",
    "get_optimal_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_variational_problem(x_0, y, n_steps, step_size, lam, tracking=None, x_gt=None, global_step=None):\n",
    "    '''\n",
    "    Solves the variational problem starting at x_0, with data term y, n_steps \n",
    "    descent steps and regularisation paremeter mu.\n",
    "    param tracking: Supports values None (no tracking), Variational (writes full tracking history into seperate dir) \n",
    "    and 'BestOnly' (writes best value over trajectory to training logger)\n",
    "    '''\n",
    "    if tracking == 'Variational':\n",
    "        p = BASE_PATH+f'{EXPERIMENT}/Logs/Step_{global_step}/Lambda_{lam}/'\n",
    "        print('Writing variational minimization to directory', p)\n",
    "        local_writer = SummaryWriter(p)\n",
    "    def add_scalar(name, value, iteration):\n",
    "        if tracking == 'Variational':\n",
    "            local_writer.add_scalar('Variational/'+name, value, iteration)\n",
    "    def add_image(name, value, iteration):\n",
    "        if tracking == 'Variational':\n",
    "            value = torch.clamp(value, 0, 1)\n",
    "            if value.shape[0] < N_IMAGES_LOGGING:\n",
    "                v = value.cpu().numpy()\n",
    "            else:\n",
    "                v = (value[:N_IMAGES_LOGGING]).cpu().numpy()\n",
    "            local_writer.add_images('Variational/'+name, v, iteration)\n",
    "            \n",
    "    best_per = -1\n",
    "    best_attained_at = -1\n",
    "    best_recon = None\n",
    "    x = x_0.detach()\n",
    "    add_image('Ground Truth', x_gt, 0)\n",
    "    for k in range(n_steps):\n",
    "        data_term = operator.forward_torch(x) - y\n",
    "        add_scalar('Data_Term', l2(data_term).detach().cpu().numpy(), k)\n",
    "        data_grad = operator.adjoint_torch(data_term)\n",
    "        add_scalar('L2_Data_Gradient', l2(data_grad).detach().cpu().numpy(), k)\n",
    "        add_scalar('Dual_Norm_Data_Gradient', torch.mean(dual(data_grad)).detach().cpu().numpy(), k)\n",
    "        if tracking  == 'Variational':\n",
    "            quality = (l2(x_gt-x)).mean().detach().cpu().numpy()\n",
    "            add_scalar('Quality', quality, k)\n",
    "            add_image('Reconstruction', x, k)\n",
    "        if not lam == 0:\n",
    "            reg_grad = regulariser.gradient(x)*GAMMA*AVERAGE_NOISE_NORM\n",
    "            add_scalar('L2_Regulariser_Gradient', l2(reg_grad).detach().cpu().numpy(), k)\n",
    "            add_scalar('Dual_Norm_Regulariser_Gradient', torch.mean(dual(reg_grad)).detach().cpu().numpy(), k)\n",
    "            x = x - step_size * (data_grad + lam*reg_grad)\n",
    "        else:\n",
    "            x = x - step_size * data_grad\n",
    "        if THRESHOLDING:\n",
    "            x = torch.clamp(x, 0, 1)\n",
    "        if tracking == 'BestOnly':\n",
    "            quality = (l2(x_gt-x)).mean().detach().cpu().numpy()\n",
    "            if quality < best_per or best_per == -1:\n",
    "                best_attained_at = k\n",
    "                best_per = quality\n",
    "                best_recon = torch.clamp(x, 0, 1).detach().cpu().numpy()[0,...]\n",
    "    if tracking == 'BestOnly':\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(best_recon[0,...])\n",
    "        plt.axis('off')\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(x_gt.cpu()[0,0,...])\n",
    "        plt.axis('off')\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(x_0.cpu()[0,0,...])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        tracker.add_scalar('Network_Training/Reconstruction_Quality', best_per, global_step)\n",
    "        tracker.add_scalar('Network_Training/Best_Attained_At', best_attained_at, global_step)\n",
    "        tracker.add_image('Network_Training/Reconstruction', best_recon, global_step)\n",
    "        tracker.add_image('Network_Training/Ground Truth', torch.clamp(x_gt[0,...], 0, 1).cpu().numpy(), global_step)\n",
    "        tracker.add_image('Network_Training/FBP', torch.clamp(x_0[0,...], 0, 1).cpu().numpy(), global_step)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(gt, fbp, sobolev=0, tracking=False, global_step=None):\n",
    "    batch_size = gt.size()[0]\n",
    "\n",
    "    # Calculate interpolation\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1).cuda()\n",
    "    alpha = alpha.expand_as(gt)\n",
    "    interpolated = Variable(alpha * gt + (1 - alpha) * fbp, requires_grad=True)\n",
    "\n",
    "    # Calculate probability of interpolated examples\n",
    "    prob_interpolated = torch.sum(regulariser(interpolated))\n",
    "\n",
    "    # Calculate gradients of probabilities with respect to examples\n",
    "    gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                           create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    gradients_norm = dual(gradients)*GAMMA*AVERAGE_NOISE_NORM\n",
    "    assert len(gradients_norm.shape) == 1 and gradients_norm.shape[0] == BATCH_SIZE\n",
    "    penalty = ((torch.nn.ReLU()(gradients_norm - 1)) ** 2).mean()\n",
    "    if tracking:\n",
    "        tracker.add_scalar('Network_Training/Gradient_Norm', gradients_norm.mean().detach().cpu().numpy(), global_step)\n",
    "        tracker.add_scalar('Network_Training/Gradient_Penalty', penalty.detach().cpu().numpy(), global_step)\n",
    "        \n",
    "    # Return gradient penalty\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = regulariser.load()\n",
    "val_gt = next(iter(val_data)).cuda()\n",
    "val_fbp, val_y = get_training_data(val_gt)\n",
    "\n",
    "while True:\n",
    "    for i, gt in enumerate(data):\n",
    "        if global_step >= TRAINING_STEPS:\n",
    "            regulariser.save(global_step)\n",
    "            break\n",
    "        gt = gt.cuda()\n",
    "        # get training data\n",
    "        fbp, y = get_training_data(gt)\n",
    "        # compute the training loss\n",
    "        loss = (regulariser(gt) - regulariser(fbp))\n",
    "        r = gradient_penalty(gt, fbp, tracking=True, global_step=global_step)\n",
    "        overall_loss = (loss + LAMBDA*r).mean()\n",
    "        tracker.add_scalar('Network_Training/Distributional_Distance', loss.mean().detach().cpu().numpy(), global_step)\n",
    "        tracker.add_scalar('Network_Training/Training_Loss', overall_loss.detach().cpu().numpy(), global_step)\n",
    "        # update network parameters\n",
    "        overall_loss.backward()\n",
    "        optimizer.step()\n",
    "        regulariser.zero_grad()\n",
    "\n",
    "        # solve the variational problem\n",
    "        if i%TRACKING_FREQ == 0:\n",
    "            solve_variational_problem(val_fbp, val_y, n_steps=N_STEPS, step_size=STEP_SIZE, lam=LAMBDA, x_gt=val_gt, \n",
    "                                      global_step=global_step, tracking='BestOnly')\n",
    "#         if global_step % (10*TRACKING_FREQ) == 0:\n",
    "#             solve_variational_problem(val_fbp, val_y, n_steps=N_STEPS, step_size=STEP_SIZE, lam=LAMBDA, x_gt=val_gt, \n",
    "#                       global_step=global_step, tracking='Variational')\n",
    "#             regulariser.save(global_step)\n",
    "        global_step += 1\n",
    "    if global_step >= TRAINING_STEPS:\n",
    "        break\n",
    "\n",
    "gt = next(iter(data)).cuda()\n",
    "fbp, y = get_training_data(gt)\n",
    "for l in [LAMBDA/3, LAMBDA, LAMBDA*3]:\n",
    "    solve_variational_problem(val_fbp, val_y, n_steps=N_STEPS, step_size=STEP_SIZE, lam=l, x_gt=val_gt, \n",
    "              global_step=global_step, tracking='Variational')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gt = next(iter(val_data)).cuda()\n",
    "val_fbp, val_y = get_training_data(val_gt)\n",
    "for l in [LAMBDA/3, LAMBDA, LAMBDA*3]:\n",
    "    solve_variational_problem(val_fbp, val_y, n_steps=N_STEPS, step_size=STEP_SIZE, lam=l, x_gt=val_gt, \n",
    "              global_step=global_step, tracking='Variational')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
