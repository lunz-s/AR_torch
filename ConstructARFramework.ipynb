{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from data import fMRIData, fMRIDataEval, LUNA, LUNAEval\n",
    "from operators import MRI, Operator, CT\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from networks import ResNet, ConvNet, ConvenientModel\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad as torch_grad\n",
    "from utils import Norm, mkdir\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves\n",
    "BASE_PATH = '/store/CCIMI/sl767/Experiments/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT = 'MRI_fMRIData_Sobolev_-5e-1_Exp_1'\n",
    "EXPERIMENT = 'CT_LUNA_Sobolev_-1_Exp_5_PS_15'\n",
    "# Data Setup\n",
    "# DATASET = 'fMRI'\n",
    "# MODALITY= 'MRI'\n",
    "# NOISE_LEVEL = 1e-3\n",
    "# N_ANGLES = 60\n",
    "# INITIAL_MINIMIZATION = False\n",
    "DATASET = 'LUNA'\n",
    "MODALITY= 'CT'\n",
    "NOISE_LEVEL = 2e-3\n",
    "N_ANGLES = 100\n",
    "INITIAL_MINIMIZATION = False\n",
    "\n",
    "# Network training\n",
    "LEARNING_RATE = 1e-4\n",
    "DECAY_FACTOR = .7\n",
    "DECAY_EVERY_NSTEPS = 5000\n",
    "APPLY_RESCALING = False\n",
    "BATCH_SIZE = 16\n",
    "TRAINING_STEPS = 15000\n",
    "# Regulariser architecture\n",
    "CHANNELS = [1, 64, 128, 128]\n",
    "DOWNSAMPLING = [False, True, True]\n",
    "SOBOLEV = -1\n",
    "PIXEL_SIZE = 15\n",
    "# regularisation parameter for gradient penalty\n",
    "MU = 1\n",
    "# scales gradient norm with average image norm. Normalizes expected distributional distance to 1.\n",
    "N_STEPS = 120\n",
    "STEP_SIZE = 1\n",
    "THRESHOLDING = False\n",
    "# tracking\n",
    "N_IMAGES_LOGGING = 1\n",
    "TRACKING_FREQ = 100\n",
    "\n",
    "# Norm scalings - a value of None will auto-set this parameter\n",
    "# Average noise norms (L2, no rescaling: 17)\n",
    "AVERAGE_NOISE_NORM = None\n",
    "AVERAGE_NOISE_DUAL = None\n",
    "AVERAGE_IMAGE_NORM = None\n",
    "AVERAGE_DUAL_NORM = None\n",
    "\n",
    "# AVERAGE_NOISE_NORM = 1\n",
    "# AVERAGE_NOISE_DUAL = 1\n",
    "# AVERAGE_IMAGE_NORM = 1 \n",
    "# AVERAGE_DUAL_NORM = 1\n",
    "\n",
    "# AVERAGE_NOISE_NORM = .5/17\n",
    "# AVERAGE_NOISE_DUAL = 1000/17\n",
    "# AVERAGE_IMAGE_NORM = 1 \n",
    "# AVERAGE_DUAL_NORM = 1\n",
    "\n",
    "parameters = {'DATASET': DATASET,\n",
    "              'MODALITY': MODALITY,\n",
    "              'NOISE_LEVEL': NOISE_LEVEL, \n",
    "              'N_ANGLES': N_ANGLES,\n",
    "              'INITIAL_MINIMIZATION': INITIAL_MINIMIZATION,\n",
    "              'LEARNING_RATE': LEARNING_RATE,\n",
    "              'APPLY_RESCALING': APPLY_RESCALING,\n",
    "              'DECAY_FACTOR': DECAY_FACTOR,\n",
    "              'DECAY_EVERY_NSTEPS' : DECAY_EVERY_NSTEPS,\n",
    "              'BATCH_SIZE': BATCH_SIZE, \n",
    "              'TRAINING_STEPS': TRAINING_STEPS, \n",
    "              'CHANNELS': CHANNELS,\n",
    "              'DOWNSAMPLING': DOWNSAMPLING,\n",
    "              'SOBOLEV': SOBOLEV,\n",
    "              'PIXEL_SIZE': PIXEL_SIZE,\n",
    "              'MU': MU,\n",
    "              'N_STEPS': N_STEPS,\n",
    "              'STEP_SIZE': STEP_SIZE,\n",
    "              'THRESHOLDING': THRESHOLDING, \n",
    "              'N_IMAGES_LOGGING': N_IMAGES_LOGGING,\n",
    "              'TRACKING_FREQ': TRACKING_FREQ\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Norm(s = SOBOLEV, c = 1/PIXEL_SIZE)\n",
    "dual = norm.dual\n",
    "plt.imshow(dual.mask[0,0,...,0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODALITY == 'MRI':\n",
    "    operator = MRI(n_directions=N_ANGLES)\n",
    "elif MODALITY == 'CT':\n",
    "    operator = CT(n_angles=N_ANGLES)\n",
    "if DATASET == 'fMRI':\n",
    "    data = DataLoader(fMRIData(), batch_size=BATCH_SIZE, num_workers=4)\n",
    "    val_data = DataLoader(fMRIDataEval(), batch_size=BATCH_SIZE)\n",
    "elif DATASET == 'LUNA':\n",
    "    data = DataLoader(LUNA(), batch_size=BATCH_SIZE, num_workers=4)\n",
    "    val_data = DataLoader(LUNAEval(), batch_size=BATCH_SIZE)\n",
    "operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(gt):\n",
    "    y = operator.add_noise(operator.forward_torch(gt), NOISE_LEVEL)\n",
    "    x_0 = operator.inverse_torch(y)\n",
    "    # potentially add a solution of variational problem at this point\n",
    "    if INITIAL_MINIMIZATION:\n",
    "        x_0 = solve_variational_problem(x_0, y, 10, .1, 0, tracking=None)\n",
    "    return x_0, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2(x):\n",
    "    return torch.mean(torch.sqrt(torch.sum(x ** 2, dim=(1,2,3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_lambda():\n",
    "    gt = next(iter(data)).cuda()\n",
    "    _, y = get_training_data(gt)\n",
    "    norm_data = torch.mean(dual(operator.adjoint_torch(operator.forward_torch(gt) - y)))\n",
    "    return norm_data.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gt = next(iter(val_data)).cuda()\n",
    "val_fbp, val_y = get_training_data(val_gt)\n",
    "\n",
    "# Find average image norms\n",
    "if AVERAGE_NOISE_NORM is None:\n",
    "    AVERAGE_NOISE_NORM = torch.mean(norm(val_gt - val_fbp)).cpu().item()\n",
    "if AVERAGE_NOISE_DUAL is None:\n",
    "    AVERAGE_NOISE_DUAL = torch.mean(dual(val_gt - val_fbp)).cpu().item()\n",
    "if AVERAGE_IMAGE_NORM is None:\n",
    "    AVERAGE_IMAGE_NORM = torch.mean(norm(val_gt)).cpu().item()\n",
    "if AVERAGE_DUAL_NORM is None:\n",
    "    AVERAGE_DUAL_NORM = torch.mean(dual(val_gt)).cpu().item()\n",
    "parameters['AVERAGE_NOISE_NORM'] = AVERAGE_NOISE_NORM\n",
    "print('AVERAGE_NOISE_NORM', AVERAGE_NOISE_NORM)\n",
    "parameters['AVERAGE_NOISE_DUAL'] = AVERAGE_NOISE_DUAL\n",
    "print('AVERAGE_NOISE_DUAL', AVERAGE_NOISE_DUAL)\n",
    "parameters['AVERAGE_IMAGE_NORM'] = AVERAGE_IMAGE_NORM\n",
    "print('AVERAGE_IMAGE_NORM', AVERAGE_IMAGE_NORM)\n",
    "parameters['AVERAGE_DUAL_NORM'] = AVERAGE_DUAL_NORM\n",
    "print('AVERAGE_DUAL_NORM', AVERAGE_DUAL_NORM)\n",
    "\n",
    "# Find optimal Lambda\n",
    "### Scale optimal lambda by 1/2\n",
    "if SOBOLEV == -1 and MODALITY=='MRI':\n",
    "    LAMBDA = get_optimal_lambda()/3\n",
    "else:\n",
    "    LAMBDA = get_optimal_lambda()\n",
    "parameters['LAMBDA'] = LAMBDA\n",
    "print('LAMBDA', LAMBDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE = True\n",
    "mkdir(BASE_PATH + EXPERIMENT)\n",
    "if os.path.exists(BASE_PATH + EXPERIMENT + '/param.json') and not OVERWRITE:\n",
    "    print('Parameters found. Not overwriting.')\n",
    "else:\n",
    "    if not os.path.exists(BASE_PATH + EXPERIMENT + '/param.json'):\n",
    "        print('Parameters not found. Creating them')\n",
    "    else:\n",
    "        print('Parameters found. OVERWRITING!')\n",
    "    with open(BASE_PATH + EXPERIMENT + '/param.json',\"w\") as f:\n",
    "        json.dump(parameters, f)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = SummaryWriter(BASE_PATH+f'{EXPERIMENT}/Logs/')\n",
    "regulariser = ResNet(channels=CHANNELS, downsamples=DOWNSAMPLING, base_path=BASE_PATH, exp_name=EXPERIMENT).cuda()\n",
    "optimizer = optim.Adam(regulariser.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=DECAY_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_variational_problem(x_0, y, n_steps, step_size, lam, tracking=None, x_gt=None, global_step=None):\n",
    "    '''\n",
    "    Solves the variational problem starting at x_0, with data term y, n_steps \n",
    "    descent steps and regularisation paremeter mu.\n",
    "    param tracking: Supports values None (no tracking), Variational (writes full tracking history into seperate dir) \n",
    "    and 'BestOnly' (writes best value over trajectory to training logger)\n",
    "    '''\n",
    "    if tracking == 'Variational':\n",
    "        p = BASE_PATH+f'{EXPERIMENT}/Logs/Step_{global_step}/Lambda_{lam}/'\n",
    "        print('Writing variational minimization to directory', p)\n",
    "        local_writer = SummaryWriter(p)\n",
    "    def add_scalar(name, value, iteration):\n",
    "        if tracking == 'Variational':\n",
    "            local_writer.add_scalar('Variational/'+name, value, iteration)\n",
    "    def add_image(name, value, iteration):\n",
    "        if tracking == 'Variational':\n",
    "            value = torch.clamp(value, 0, 1)\n",
    "            if value.shape[0] < N_IMAGES_LOGGING:\n",
    "                v = value.cpu().numpy()\n",
    "            else:\n",
    "                v = (value[:N_IMAGES_LOGGING]).cpu().numpy()\n",
    "            local_writer.add_images('Variational/'+name, v, iteration)\n",
    "            \n",
    "    best_per = -1\n",
    "    best_attained_at = -1\n",
    "    best_recon = None\n",
    "    x = x_0.detach()\n",
    "    add_image('Ground Truth', x_gt, 0)\n",
    "    for k in range(n_steps):\n",
    "        data_term = operator.forward_torch(x) - y\n",
    "        add_scalar('Data_Term', l2(data_term).detach().cpu().numpy(), k)\n",
    "        data_grad = operator.adjoint_torch(data_term)\n",
    "        add_scalar('L2_Data_Gradient', l2(data_grad).detach().cpu().numpy(), k)\n",
    "        add_scalar('Dual_Norm_Data_Gradient', torch.mean(dual(data_grad)).detach().cpu().numpy(), k)\n",
    "        if tracking  == 'Variational':\n",
    "            quality = (l2(x_gt-x)).mean().detach().cpu().numpy()\n",
    "            add_scalar('Quality', quality, k)\n",
    "            add_image('Reconstruction', x, k)\n",
    "        if not lam == 0:\n",
    "            reg_grad = regulariser.gradient(x)\n",
    "            add_scalar('L2_Regulariser_Gradient', l2(reg_grad).detach().cpu().numpy(), k)\n",
    "            add_scalar('Dual_Norm_Regulariser_Gradient', torch.mean(dual(reg_grad)).detach().cpu().numpy(), k)\n",
    "            x = x - step_size * (data_grad + lam*reg_grad)\n",
    "        else:\n",
    "            x = x - step_size * data_grad\n",
    "        if THRESHOLDING:\n",
    "            x = torch.clamp(x, 0, 1)\n",
    "        if tracking == 'BestOnly':\n",
    "            quality = (l2(x_gt-x)).mean().detach().cpu().numpy()\n",
    "            if quality < best_per or best_per == -1:\n",
    "                best_attained_at = k\n",
    "                best_per = quality\n",
    "                best_recon = torch.clamp(x, 0, 1).detach().cpu().numpy()[0,...]\n",
    "    if tracking == 'BestOnly':\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(best_recon[0,...])\n",
    "        plt.axis('off')\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(x_gt.cpu()[0,0,...])\n",
    "        plt.axis('off')\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(x_0.cpu()[0,0,...])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        tracker.add_scalar('Network_Training/Reconstruction_Quality', best_per, global_step)\n",
    "        tracker.add_scalar('Network_Training/Best_Attained_At', best_attained_at, global_step)\n",
    "        tracker.add_image('Network_Training/Reconstruction', best_recon, global_step)\n",
    "        tracker.add_image('Network_Training/Ground Truth', torch.clamp(x_gt[0,...], 0, 1).cpu().numpy(), global_step)\n",
    "        tracker.add_image('Network_Training/FBP', torch.clamp(x_0[0,...], 0, 1).cpu().numpy(), global_step)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(gt, fbp, sobolev=0, tracking=False, global_step=None):\n",
    "    batch_size = gt.size()[0]\n",
    "\n",
    "    # Calculate interpolation\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1).cuda()\n",
    "    alpha = alpha.expand_as(gt)\n",
    "    interpolated = Variable(alpha * gt + (1 - alpha) * fbp, requires_grad=True)\n",
    "\n",
    "    # Calculate probability of interpolated examples\n",
    "    prob_interpolated = torch.sum(regulariser(interpolated))\n",
    "\n",
    "    # Calculate gradients of probabilities with respect to examples\n",
    "    gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                           create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    gradients_norm = dual(gradients)\n",
    "    assert len(gradients_norm.shape) == 1 and gradients_norm.shape[0] == BATCH_SIZE\n",
    "    penalty = ((torch.nn.ReLU()(gradients_norm - 1)) ** 2).mean()\n",
    "    if tracking:\n",
    "        tracker.add_scalar('Network_Training/Gradient_Norm', gradients_norm.mean().detach().cpu().numpy(), global_step)\n",
    "        tracker.add_scalar('Network_Training/Gradient_Penalty', penalty.detach().cpu().numpy(), global_step)\n",
    "        \n",
    "    # Return gradient penalty\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = regulariser.load()\n",
    "val_gt = next(iter(val_data)).cuda()\n",
    "val_fbp, val_y = get_training_data(val_gt)\n",
    "\n",
    "while True:\n",
    "    for i, gt in enumerate(data):\n",
    "        if global_step >= TRAINING_STEPS:\n",
    "            regulariser.save(global_step)\n",
    "            break\n",
    "        gt = gt.cuda()\n",
    "        # get training data\n",
    "        fbp, y = get_training_data(gt)\n",
    "        # compute the training loss\n",
    "        loss = (regulariser(gt) - regulariser(fbp))/AVERAGE_NOISE_NORM\n",
    "        r = gradient_penalty(gt, fbp, tracking=True, global_step=global_step)\n",
    "        overall_loss = (loss + MU*r).mean()\n",
    "        tracker.add_scalar('Network_Training/Distributional_Distance', loss.mean().detach().cpu().numpy(), global_step)\n",
    "        tracker.add_scalar('Network_Training/Training_Loss', overall_loss.detach().cpu().numpy(), global_step)\n",
    "        tracker.add_scalar('Network_Training/Learning_Rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "        # update network parameters\n",
    "        overall_loss.backward()\n",
    "        optimizer.step()\n",
    "        regulariser.zero_grad()\n",
    "\n",
    "        # solve the variational problem\n",
    "        if i%TRACKING_FREQ == 0:\n",
    "            solve_variational_problem(val_fbp, val_y, n_steps=N_STEPS, step_size=STEP_SIZE, lam=LAMBDA, x_gt=val_gt, \n",
    "                                      global_step=global_step, tracking='BestOnly')\n",
    "        if (i%DECAY_EVERY_NSTEPS) == DECAY_EVERY_NSTEPS-1:\n",
    "            lr_scheduler.step()\n",
    "            print('Decay step taken')\n",
    "        global_step += 1\n",
    "    if global_step >= TRAINING_STEPS:\n",
    "        break\n",
    "\n",
    "gt = next(iter(data)).cuda()\n",
    "fbp, y = get_training_data(gt)\n",
    "for l in [LAMBDA/3, LAMBDA, LAMBDA*3]:\n",
    "    solve_variational_problem(val_fbp, val_y, n_steps=N_STEPS, step_size=STEP_SIZE, lam=l, x_gt=val_gt, \n",
    "              global_step=global_step, tracking='Variational')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gt = next(iter(val_data)).cuda()\n",
    "val_fbp, val_y = get_training_data(val_gt)\n",
    "for l in [LAMBDA/3, LAMBDA, LAMBDA*3]:\n",
    "    solve_variational_problem(val_fbp, val_y, n_steps=N_STEPS, step_size=STEP_SIZE, lam=l, x_gt=val_gt, \n",
    "              global_step=global_step, tracking='Variational')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
